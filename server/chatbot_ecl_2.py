# -*- coding: utf-8 -*-
"""chatbot_ecl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14YHXLj-ayzBHJ9T7yBlYUyiEracFeHDv
"""

from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random
import numpy as np

import torch
import torch.nn as nn
from torch.autograd import Variable
from torch import optim
import torch.nn.functional as F
import os


dirname=os.path.dirname(__file__)

NEW_DATA_FILE=os.path.join(dirname,'./new_data/')
DATA_FILE=os.path.join(dirname,'./data.txt')
ENCODER_FILE= os.path.join(dirname,'./encoder_25000_stem_ns_para.pkl')
DECODER_FILE= os.path.join(dirname,'./decoder_25000_stem_ns_para.pkl')


"""# New Section"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from nltk.stem.snowball import FrenchStemmer
stemmer = FrenchStemmer()
stemmer.stem('animaux')



"""(1) Create Lang class represente the dictionaire"""

SOS_token = 0
EOS_token = 1

class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {"Null" :2}
        self.word2count = {}#count the frenquence of a word appear in the document
        self.index2word = {0: "SOS", 1: "EOS", 2:"Null"} # SOS : start of sentence; EOS: end of sentence; 
        # Null : word doesn't exist in the traning data.
        self.n_words = 3  # Count SOS and EOS and Null

    def addSentence(self, sentence):
        ''' add  a sentence to the class'''
        for word in sentence.split():
            if word == '':
                print('****************',sentence)
            self.addWord(word)
         
    def addWord(self, word):
        ''' add a word to the class '''
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

"""(2) Transform french to unicode and normalize the sentence"""

# Turn a Unicode string to plain ASCII, thanks to
# http://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters except digits
def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"(['\'()/&!{}\-\’])", r" ", s)
    #s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    #s = re.sub(r"[^a-zA-Z0-9?]+", r" ", s)
    #s = re.sub(r"[^a-zA-Z0-9?&\'\’\%\-]+", r" ", s)
    #s = re.sub(r"[^a-zA-Z0-9?&\%\-]+", r" ", s)
    return s

# s="Il eût été bien de l'inscrire en 2017 !! N'est-ce pas c’est dites-moi?"
# print(unicodeToAscii(s))
# s = normalizeString(s)
# print(s)

import os
os.getcwd()

from nltk.stem.snowball import FrenchStemmer
stemmer = FrenchStemmer()
stemmer.stem('quel est adresse mail vp challenge bdr bureau residences ?')

def stemString(s,stemmer):
  new_str = ""
  for i in s.split(' '):
    new_str = new_str + " " + stemmer.stem(i)
  
  return new_str

stemString('quel est adresse mail vp challenge bdr bureau residences ?',stemmer)

a = stemString("animaux sauvages",stemmer)
# print(a)
# a = normalizeString(a)
# print(a)

"""(3) Load data and create two dict : question and answer respectively"""

def readLangs(questions, answers, reverse=False,stem=False):
    # print("Reading lines...")
        
    lines = open(DATA_FILE, encoding='utf-8').\
        read().strip().split('\n')
    # Split every line into pairs and normalize
    pairs = [[s for s in l.split('\t')] for l in lines]
    if stem:
        for pair in pairs:
            pair[0] = stemString(pair[0],stemmer)
            pair[0] = normalizeString(pair[0])
            pair[1] = normalizeString(pair[1])
    else : 
        for pair in pairs:
            pair[0] = normalizeString(pair[0])
            pair[1] = normalizeString(pair[1])
            
    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(answers)
        output_lang = Lang(questions)
    else:
        input_lang = Lang(questions)
        output_lang = Lang(answers)

    return input_lang, output_lang, pairs

"""# New Section"""

MAX_LENGTH = 25
# we use stopwords to delete those words not meaningfull
stopwords = 'de du des d le la les l ce c ci ça m me ma si t sur n en s si a y au un une on il nous vous je j a b c d e r f avoir '.split()

#stopwords = []

def TrimWordsSentence(sentence):
    resultwords = [word for word in sentence.split() if word.lower() not in stopwords]
    resultwords = ' '.join(resultwords)
    return resultwords

def TrimWords(pairs):
    for pair in pairs: 
        pair[0] = TrimWordsSentence(pair[0])
        pair[1] = TrimWordsSentence(pair[1])
    return pairs

# delete longer sentences
def filterPair(p):
    return len(p[0].split()) < MAX_LENGTH and \
        len(p[1].split()) < MAX_LENGTH 

def filterPairs(pairs):
    return [pair for pair in pairs if filterPair(pair)]

def prepareData(lang1, lang2, reverse=False,stem=False):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse,stem)
    # print("Read %s sentence pairs" % len(pairs))
    pairs = TrimWords(pairs)
    
    for pair in [pair for pair in pairs if not filterPair(pair)]:
        print('%s (%d) -> %s (%d)' % (pair[0],len(pair[0].split()),pair[1],len(pair[1].split())))  
    
    pairs = filterPairs(pairs)
    
    # print('')
    # print("Trimmed to %s sentence pairs" % len(pairs))
    # print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
        if '' in output_lang.word2index: print(pair[1].split())
    # print("Counted words:")
    # print(input_lang.name, input_lang.n_words)
    # print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs

input_lang, output_lang, pairs = prepareData('questions', 'answers', reverse=False,stem=True)

# for i in pairs[0:10]:
#   print(i[0])
#   print(i[1])

import os
os.getcwd()

class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size, n_layers=1):
        super(EncoderRNN, self).__init__()
        self.n_layers = n_layers
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        for i in range(self.n_layers):
            output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        result = Variable(torch.zeros(1, 1, self.hidden_size))
        return result

class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)))#, dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        for i in range(self.n_layers):
            output = F.relu(output)
            output, hidden = self.gru(output, hidden)

        output = F.log_softmax(self.out(output[0]))#, dim=1)
        return output, hidden, attn_weights

    def initHidden(self):
        result = Variable(torch.zeros(1, 1, self.hidden_size))
        return result



def indexesFromSentence(lang, sentence,MAX_LENGTH=25):
    words = sentence.split()
    if len(words)>MAX_LENGTH:
        new_words = random.choices(words,k=MAX_LENGTH)
    else:
        new_words = words
    
    
    a = []
    for word in new_words:
        try:
            a.append(lang.word2index[word])
        except KeyError:
            a.append(lang.word2index['Null'])
    
    return a

def variableFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    result = Variable(torch.LongTensor(indexes).view(-1, 1))
    return result

def variablesFromPair(pair):
    input_variable = variableFromSentence(input_lang, pair[0])
    target_variable = variableFromSentence(output_lang, pair[1])
    return (input_variable, target_variable)

teacher_forcing_ratio = 0.5

def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_variable.size()[0]
    target_length = target_variable.size()[0]

    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(
            input_variable[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0][0]

    decoder_input = Variable(torch.LongTensor([[SOS_token]]))

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            
            #decoder_output, decoder_hidden = decoder(
             #   decoder_input, decoder_hidden)
            
            loss += criterion(decoder_output, target_variable[di])
            decoder_input = target_variable[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            
            #decoder_output, decoder_hidden = decoder(
             #   decoder_input, decoder_hidden)       
            
            topv, topi = decoder_output.data.topk(1)
            ni = topi[0][0].item()

            decoder_input = Variable(torch.LongTensor([[ni]]))
            decoder_input = decoder_input

            loss += criterion(decoder_output, target_variable[di])
            if ni == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    #return loss.data[0] / target_length
    return loss.item() / target_length



import time
import math

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

def trainIters(encoder, decoder,pairs, n_iters, print_every=1000, plot_every=100, learning_rate=0.01,criterion = nn.NLLLoss(),save_name=""):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    
    training_pairs = [variablesFromPair(random.choice(pairs))
                      for i in range(n_iters)]
   # criterion = nn.NLLLoss()
    #criterion = nn.CrossEntropyLoss()
    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_variable = training_pair[0]
        target_variable = training_pair[1]

        loss = train(input_variable, target_variable, encoder,
                     decoder, encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0
        if iter % 5000 ==0:
            torch.save(encoder.state_dict(),'encoder_{}_{}_para.pkl'.format(iter,save_name))
            torch.save(decoder.state_dict(),'decoder_{}_{}_para.pkl'.format(iter,save_name))

    showPlot(plot_losses)

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np


def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)

"""Negative sampling"""

## Here we create a sentence randomly.
## We need a dictionaire to find the frequence of a word 
## We can define the length of the sentence we need
## We use the ratio between the occurrence of a word and the total occurrence as the threshold
## Then we use the 0-1 distribution model. If the ratio behind 0.01, we use 0.01 as threshold.
def create_sentence(dict_w2c,sentence_length):
    length_word = len(dict_w2c)
    nom_tot = sum(list(dict_w2c.values()))
    words = list(dict_w2c.keys())
    values = list(dict_w2c.values())
    output_sentence = []

    while(len(output_sentence)<sentence_length-1):
        #print(output_sentence)
        i2d = random.randint(0,length_word-1)
        val = values[i2d]
        if (val/nom_tot<0.01):
            if(random.random()<0.01) and words[i2d]!="?":
                output_sentence.append(words[i2d])
        else:
            if (random.random()< val/nom_tot) and words[i2d]!="?":
                output_sentence.append(words[i2d])
    sentence = ' '.join(output_sentence)
    sentence+=' ?'
    return sentence

create_sentence(input_lang.word2count,4)

create_sentence(input_lang.word2count,4)
def create_pair(input_sentence):
    pair = [input_sentence,"je ne comprend pas"]
    return pair

def add_newpair(pairs,word2count,nomb=100,length_sent=7):
    for i in range(nomb):
        input_sentence = create_sentence(word2count,length_sent)
        pair = create_pair(input_sentence)
        pairs.append(pair)
    return pairs

new_sentence = "je ne comprend pas"
output_lang.addSentence(new_sentence)
new_sentence = stemString(new_sentence,stemmer)
new_sentence = normalizeString(new_sentence)
#new_sentence = stemString(new_sentence)
new_sentence = TrimWordsSentence(new_sentence)
# print(new_sentence)
input_lang.addSentence(new_sentence)

## We add 30 sentences to the pairs
# print(len(pairs))
# print(pairs[-1])
number = 30
paris = add_newpair(pairs,input_lang.word2count,nomb=number)
# print(len(pairs))
# print(pairs[-1])

stemString("etudiants",stemmer)

hidden_size = 256
#hidden_size = 100
encoder = EncoderRNN(input_lang.n_words, hidden_size)

decoder = AttnDecoderRNN(hidden_size, output_lang.n_words,1, dropout_p=0.1)

#trainIters(encoder, decoder,pairs,26000,save_name="stem_ns", print_every=200,learning_rate=0.01)

encoder.load_state_dict(torch.load(ENCODER_FILE))
decoder.load_state_dict(torch.load(DECODER_FILE))

def evaluate(encoder, decoder,sentence, max_length=MAX_LENGTH,stem=False):
    sentence = unicodeToAscii(sentence)
    if stem:
        sentence = stemString(sentence,stemmer)
    sentence = normalizeString(sentence)
    sentence = TrimWordsSentence(sentence)
    input_variable = variableFromSentence(input_lang, sentence)
    input_length = input_variable.size()[0]
    encoder_hidden = encoder.initHidden()

    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(input_variable[ei],
                                                 encoder_hidden)
        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]

    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS

    decoder_hidden = encoder_hidden

    decoded_words = []
    decoder_attentions = torch.zeros(max_length, max_length)

    for di in range(max_length):
        
        #decoder_output, decoder_hidden = decoder(
         #   decoder_input, decoder_hidden)

        decoder_output, decoder_hidden, decoder_attention = decoder(
            decoder_input, decoder_hidden, encoder_outputs)
        decoder_attentions[di] = decoder_attention.data

        topv, topi = decoder_output.data.topk(1)
        ni = topi[0][0].item()

        if ni == EOS_token:
            decoded_words.append('<EOS>')
            break
        else:
            decoded_words.append(output_lang.index2word[ni])

        decoder_input = Variable(torch.LongTensor([[ni]]))
    print("norm_question : {}".format(sentence))
    return decoded_words, decoder_attentions[:di + 1]
    #return decoded_words

def evaluateRandomly(encoder, decoder, n=10,stem=False):
    for i in range(n):
        pair = random.choice(pairs)
        print('>', pair[0])
        print('=', pair[1])
        
        output_words, attentions = evaluate(encoder, decoder, pair[0],stem=stem)
        #output_words = evaluate(encoder, decoder, pair[0])
     
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')

# evaluateRandomly(encoder, decoder,n=5,stem=True)

"""Beam Search"""

import copy
def beam_search(encoder,decoder,sentence,beam_size,max_length = MAX_LENGTH,stem=False):
    sentence = unicodeToAscii(sentence)
    if stem:
        sentence=stemString(sentence,stemmer)
    sentence = normalizeString(sentence)
    sentence = TrimWordsSentence(sentence)
    input_variable = variableFromSentence(input_lang, sentence)
    input_length = input_variable.size()[0]
    encoder_hidden = encoder.initHidden()

    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(input_variable[ei],
                                                 encoder_hidden)
        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]

    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS

    decoder_hidden = encoder_hidden

    decoded_words = []
    decoder_attentions = torch.zeros(max_length, max_length)

    decoder_output, decoder_hidden, decoder_attention = decoder(
        decoder_input, decoder_hidden, encoder_outputs)

    topv,topi = decoder_output.data.topk(beam_size)

    new_decoder_hidden = decoder_hidden.expand(beam_size,*decoder_hidden.size())

    prevs_words = torch.zeros(beam_size*beam_size,MAX_LENGTH)


    prevs_words[:,0] = SOS_token
    scores = torch.zeros(beam_size*beam_size)
    output_sentence = []
    out_scores = []
    step = 1
    for i in range(beam_size):
        scores[i*beam_size:(i+1)*beam_size] = topv[0][i]
        prevs_words[i*beam_size:(i+1)*beam_size,1] = topi[0][i].item()
        
    while(len(out_scores)<beam_size and step<20):
        for i in range (beam_size):
            decoder_input = Variable(torch.LongTensor([[prevs_words[i*beam_size,step]]]))
            decoder_hidden = new_decoder_hidden[i]
            decoder_output, decoder_hidden, decoder_attention = decoder(
            decoder_input, decoder_hidden, encoder_outputs)
            topv,topi = decoder_output.data.topk(beam_size)
            new_decoder_hidden[i] = decoder_hidden
            for j in range(beam_size):
                prevs_words[j+i*beam_size,step+1] = topi[0][j].item()

                scores[j+i*beam_size] +=topv[0][j]



        s_topv,s_topi = scores.topk(beam_size)    
       # print(scores)
        #print(s_topi)
        prevs_words_copy = copy.deepcopy(prevs_words)
        new_decoder_hidden_copy = torch.zeros(*new_decoder_hidden.size())
        for i in range(beam_size):
            scores[i*beam_size:(i+1)*beam_size] = s_topv[i]
            prevs_words[i*beam_size:(i+1)*beam_size,:] = prevs_words_copy[s_topi[i],:]
            new_decoder_hidden_copy[i] = new_decoder_hidden[s_topi[i]//beam_size]
        new_decoder_hidden = new_decoder_hidden_copy
        for i in range(beam_size):
            if prevs_words[i*beam_size,step+1]==EOS_token and scores[i*beam_size].item()>-100:
               # print(prevs_words[i*beam_size,:],scores[i*beam_size])
                a = prevs_words[i*beam_size,:].numpy()
                b = copy.deepcopy(a)
                output_sentence.append(b)
                #print(output_sentence)
                out_scores.append(scores[i*beam_size].item())
                scores[i*beam_size:(i+1)*beam_size] = -500


        #print(step)     
        step+=1
    answer = ''
    out_sentence = output_sentence[out_scores.index(max(out_scores))]
    for i in out_sentence:
        token = output_lang.index2word[i.item()]
        if token == "EOS":
            break
        if token !="SOS":
            answer = answer + ' ' + token
   # print("question : {}\n".format(sentence))
   # print("bot : {}\n".format(answer))
    return answer

def Norm(ques):
    if ques[-1] =="?":
        ques = ques[0:-1]+" ?"
    return ques

def chat(encoder,decoder,sentence,beam_size=1,stem=False):
    sentence = Norm(sentence)
    answer = beam_search(encoder,decoder,sentence,beam_size,stem=stem)
    # print("question : {}\n".format(sentence))
    print(answer)

s1 = "iltezata y combienrtare de  du  tcsss1 ze zettzveryreytrey ?"
s2 = "il combien de cours y du mth tc1 ?"
s3 = "cours combien mth tc1 ?"
s4 = "combien cours mth tc3 ?"

# chat(encoder,decoder,s1,beam_size=4,stem=True)
# chat(encoder,decoder,s2,beam_size=1,stem=True)
# chat(encoder,decoder,s3,beam_size=1,stem=True)
# chat(encoder,decoder,s4,beam_size=3,stem=True)





"""Retrieve Model"""

import nltk
import numpy as np
import random
import string
from io import open
import unicodedata
import re
import random

def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"(['\'()/&!{}\-\’||,◊:.])", r" ", s)
    #s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    #s = re.sub(r"[^a-zA-Z0-9?]+", r" ", s)
    #s = re.sub(r"[^a-zA-Z0-9?&\'\’\%\-]+", r" ", s)
    #s = re.sub(r"[^a-zA-Z0-9?&\%\-]+", r" ", s)
    return s

stopwords = 'de du des d le la les l ce c ci ça m me ma si t sur n en s si a y au un une on il nous vous je j a b c d e r f avoir '.split()

#stopwords = []

def TrimWordsSentence(sentence):
    resultwords = [word for word in sentence.split() if word.lower() not in stopwords]
    resultwords = ' '.join(resultwords)
    return resultwords

from nltk.stem.snowball import FrenchStemmer
stemmer = FrenchStemmer()

def stemString(s,stemmer):
    new_str = ""
    for i in s.split(' '):
        new_str = new_str + " " + stemmer.stem(i)
    return new_str

os.getcwd()

import os
path = NEW_DATA_FILE
files = os.listdir(path)
# print (files)

match_dict = {}
all_doc = []
i = 0
for file in files: 

  if file.split(".")[-1]!="ipynb_checkpoints":
    doc = open(dirname+'/new_data/{}'.format(file), encoding='utf-8').read().strip()
    match_dict["{}".format(i)] = file
    all_doc.append(doc)
    i +=1

match_dict

match_dict['ue'] = "https://drive.google.com/open?id=1zwQMGZFFG2fUhMiEJB3aiPMjXAQJ7lhq"

all_doc

def LemNormalize(doc):
    doc = stemString(doc,stemmer)
    doc = unicodeToAscii(doc)
    doc = normalizeString(doc)
    doc = TrimWordsSentence(doc)
    words = nltk.word_tokenize(doc)
    return words

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# import nltk
# nltk.download('punkt')

import copy

def key_search(question,all_doc,match_dict):
    question = stemString(question,stemmer)
    question = unicodeToAscii(question)
    question = normalizeString(question)
    question = TrimWordsSentence(question)
    new_doc = copy.deepcopy(all_doc)
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)
    new_doc.append(question)
    tfidf = TfidfVec.fit_transform(new_doc)
    vals = cosine_similarity(tfidf[-1],tfidf)
    ind = vals.argsort()[0][-3:-1]
    for i in ind:
        answer = match_dict["{}".format(i)]
        doc_site = match_dict["{}".format(answer.split("_")[0])]
        print("you can find the whole document here ; {} in {}".format(doc_site,answer.split("_")[-1].split(".")[0]))
    return ind



# val = key_search("inf tc2",all_doc,match_dict)  




import sys

text_from_node_server=str(sys.argv[1])

chat(encoder,decoder,text_from_node_server,beam_size=4,stem=True)

sys.stdout.flush()
















